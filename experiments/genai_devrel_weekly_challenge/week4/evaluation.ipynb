{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Gen AI Evaluation SDK to Evaluate Models\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fevaluation%2Fevaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv run pip install -U -q google-cloud-aiplatform[evaluation]\n",
    "%uv run pip install -U -q datasets\n",
    "%uv run pip install -U -q google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.evaluation import (\n",
    "    EvalTask,\n",
    "    MetricPromptTemplateExamples,\n",
    "    PairwiseMetric,\n",
    ")\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.preview.evaluation import notebook_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an evaluation dataset\n",
    "\n",
    "Load a subset of the `OpenOrca` dataset using the `huggingface/datasets` library. We will use 100 samples from the first 100 rows of the \"train\" split of `OpenOrca` dataset to demonstrate evaluating prompts and model responses in this Colab.\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "The OpenOrca dataset is a collection of augmented [FLAN Collection data](https://arxiv.org/abs/2301.13688). Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope. The data is primarily used for training and evaluation in the field of natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = (\n",
    "    load_dataset(\n",
    "        \"Open-Orca/OpenOrca\",\n",
    "        data_files=\"1M-GPT4-Augmented.parquet\",\n",
    "        split=\"train[:100]\",\n",
    "    )\n",
    "    .to_pandas()\n",
    "    .drop(columns=[\"id\"])\n",
    "    .rename(columns={\"response\": \"reference\"})\n",
    ")\n",
    "\n",
    "dataset = ds.sample(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model to be Evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model_id = \"gemini-2.0-flash\"  # @param {type:\"string\"}\n",
    "second_model_id = \"[your-gemini-1.5-finetuned-endpoint-URI]\"  # @param {type:\"string\"}\n",
    "\n",
    "first_model = GenerativeModel(\n",
    "    \"gemini-2.0-flash\",\n",
    "    generation_config={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "second_model = GenerativeModel(\n",
    "    \"projects/109790610330/locations/us-central1/endpoints/6532745037896220672\",\n",
    "    generation_config={\"temperature\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Detailed of available metrics can be viewed [here](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Evaluation\n",
    "\n",
    "This evaluation type produce a score for each evaluated metrics. E.g. 1-5 in which higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an EvalTask with a list of metrics\n",
    "pointwise_eval_task = EvalTask(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        \"groundedness\",\n",
    "        \"instruction_following\",\n",
    "        \"question_answering_quality\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "first_model_result = pointwise_eval_task.evaluate(\n",
    "    model=first_model,\n",
    "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
    ")\n",
    "\n",
    "second_model_result = pointwise_eval_task.evaluate(\n",
    "    model=second_model,\n",
    "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_utils.display_eval_result(\n",
    "    first_model, title=\"Gemini 2.0 Flash Evaluation Results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_utils.display_eval_result(\n",
    "    second_model, title=\"Gemini 1.5 Flash Finetuned Evaluation Results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Evaluation\n",
    "\n",
    "This evaluation type will directly compete two models and show the winning rate of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an EvalTask with a list of metrics\n",
    "listed_metrics = [\"groundedness\", \"instruction_following\", \"question_answering_quality\"]\n",
    "\n",
    "pairwise_metrics = [\n",
    "    PairwiseMetric(\n",
    "        metric=metric,\n",
    "        metric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(metric),\n",
    "        # Define a baseline model to compare against\n",
    "        baseline_model=first_model,\n",
    "    )\n",
    "    for metric in listed_metrics\n",
    "]\n",
    "\n",
    "pairwise_result = EvalTask(\n",
    "    dataset=dataset,\n",
    "    metrics=pairwise_metrics,\n",
    ").evaluate(\n",
    "    model=second_model,\n",
    "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_utils.display_eval_result(\n",
    "    pairwise_result, title=\"Gemini 2.0 Flash vs Gemini 1.5 Flash Finetuned Results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check detailed explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_utils.display_explanations(pairwise_result, num=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
